
import asyncio
import json

# Simulaci√≥n de la clase base
class AIProviderAdapter:
    pass

# Copia exacta de la l√≥gica modificada (sin imports externos)
class LocalFallbackProvider(AIProviderAdapter):
    """
    Proveedor de respaldo local.
    Nunca falla. Devuelve contenido gen√©rico estructurado.
    √ötil para desarrollo offline o cuando se acaban los cr√©ditos.
    """
    name: str = "local_fallback"
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """
        Devuelve un JSON v√°lido simulado, adapt√°ndose al contexto del prompt.
        """
        
        # Detecci√≥n robusta de contexto: Gu√≠a vs Generaci√≥n de Posts
        prompt_lower = prompt.lower()
        is_guide_request = (
            "guideoption" in prompt_lower or 
            "message" in prompt_lower or 
            "state_patch" in prompt_lower or
            "modo colaborador" in prompt_lower or
            "modo experto" in prompt_lower or
            "modo gu√≠a" in prompt_lower or
            "arapost manager" in prompt_lower
        )
        
        if is_guide_request:
            # Intentar extraer el input del usuario para personalizar la respuesta (Simulaci√≥n de "eco")
            # Buscamos "INPUT USUARIO:\n" o similar
            user_echo = ""
            try:
                if "INPUT USUARIO:" in prompt:
                    parts = prompt.split("INPUT USUARIO:")
                    if len(parts) > 1:
                        # Tomar la siguiente l√≠nea o el contenido entre comillas
                        raw_input = parts[1].strip().split("\n")[0].strip('"')
                        if raw_input and len(raw_input) > 2:
                            user_echo = f" (Entendido: '{raw_input}')"
            except:
                pass

            fallback_guide = {
                "message": f"¬°Hola! üëã Parece que mi conexi√≥n neuronal (IA) est√° inestable, pero te escucho{user_echo}. Cu√©ntame m√°s detalles sobre tu objetivo para poder avanzar.",
                "options": [
                    {"label": "Continuar", "value": "continue"},
                    {"label": "Reintentar conexi√≥n", "value": "retry"}
                ],
                "state_patch": {},
                "updated_summary": "Conversaci√≥n en modo respaldo local."
            }
            return json.dumps(fallback_guide, ensure_ascii=False)
        
        # Default: Fallback para generaci√≥n de posts
        fallback_content = {
            "title": "Contenido Generado (Modo Fallback)",
            "content": "Este es un contenido generado autom√°ticamente en modo de respaldo. "
                       "El proveedor principal de IA no estaba disponible. "
                       "Por favor, edite este borrador antes de aprobarlo.",
            "hashtags": ["#Fallback", "#AraAutoPublisher", "#ModoSeguro"],
            "cta": "Revise la configuraci√≥n de IA",
            "platform": "linkedin"
        }
        
        return json.dumps(fallback_content, ensure_ascii=False)

async def test_fallback():
    provider = LocalFallbackProvider()
    
    # Prompt t√≠pico de colaborador
    prompt = """
    Eres AraPost Manager en MODO COLABORADOR.
    INPUT USUARIO:
    "Hola, soy Wily y quiero vender zapatos"
    FORMATO RESPUESTA (JSON): { "message": ... }
    """
    
    print(f"Testing prompt with length: {len(prompt)}")
    response = await provider.generate(prompt)
    print("\n--- Response ---")
    print(response)
    
    try:
        data = json.loads(response)
        print("\n‚úÖ Valid JSON")
        print(f"Message: {data.get('message')}")
        
        if "Wily" in data.get('message'):
             print("‚úÖ User input echoed correctly")
        else:
             print("‚ùå User input NOT echoed")
             
    except Exception as e:
        print(f"\n‚ùå Invalid JSON: {e}")

if __name__ == "__main__":
    asyncio.run(test_fallback())
